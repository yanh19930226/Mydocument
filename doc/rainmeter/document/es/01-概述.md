# ============== elasticsearch ==========

## 1 概述

Elaticsearch，简称为es，是一个开源的高扩展的**分布式全文检索引擎**，它可以近乎实时的存储、检索数据，是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单

## 2 安装

### 2.1 安装 elasticsearch

https://mirrors.huaweicloud.com/elasticsearch/7.6.2/?C=N&O=D

**查看目录结构**

```text
bin 启动文件 
config 配置文件 
	log4j2 日志配置文件 
	jvm.options  虚拟机相关的配置 
	elasticsearch.yml elasticsearch 的配置文件！ 默认 9200 端口！！ 
lib 相关jar包 
logs 日志！ 
modules 功能模块 
plugins 插件！
```

**启动**

在linux中es是不能以root用于启动的，启动将会报错

```text
can not run elasticsearch as root
```

添加一个es的用户，并授权

```js
## 添加用户
adduser elasticsearch	

## 设置密码
passwd elasticsearch

## 授权
chown -R elasticsearch /usr/local/elasticsearch-7.6.2
```

切换至新用户并以后台形式启动es

```text
su elasticsearch
./elasticsearch -d
```

**访问**

http://localhost:9200

访问默认端口，返回 **JSON**则启动成功

### 2.2 安装可视化插件

https://github.com/mobz/elasticsearch-head/

在nodejs环境下执行即可

~~~
cnpm install
cnpm run start
~~~

**访问**

http://localhost:9100

如果出现跨域问题，修改elasticsearch.yml配置文件，添加以下内容

```yml
http.cors.enabled: true 
http.cors.allow-origin: "*"
```

到此，es 和 es的可视化工具就完成了。

### 2.3 安装Kibana

https://mirrors.huaweicloud.com/kibana/7.6.2/?C=N&O=D

**访问**

[http://localhost:5601](http://localhost:5601/) 

**汉化**

```yml
 i18n.locale: "zh-CN"
```

### 2.4 安装ik分词器

即把一段中文或者别的划分成一个个的关键字，我们在搜索时候会把自己的信息进行分词，会把数据库中或者索引库中的数据进行分词，然后进行一个匹配操作，默认的中文分词是将每个字看成一个词。

**IK提供了两个分词算法**

ik_smart 和 ik_max_word，其中 ik_smart 为最少切分，ik_max_word为最细 粒度划分

https://github.com/medcl/elasticsearch-analysis-ik/releases

解压后放入到es的plugin目录下即可，重启观察ES，可以看到ik分词器被加载了！